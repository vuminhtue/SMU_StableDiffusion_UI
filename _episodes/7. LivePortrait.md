---
title: "Install LivePortrait to M3/SPOD"
teaching: 15 min
exercises: 0
questions: "Install LivePortrait to M3/SPOD"
objectives:

keypoints:
- "Foocus"
---
# 7. Install LivePortrait to M3/SPOD

- LivePortrait is open source package: https://github.com/KwaiVGI/LivePortrait

- The following steps instruct you to install it on SMU M3/SPOD to shared folder between them.

- Here, I have a shared folder under ColdFront allocation that could be accessed by both M3 (gpu_dev) & SPOD.

- Note, at the time of writing this instruction cuda/11.8 is recommended with gcc/11 for GPU usage

## 1. Create conda env

Instruction can be found in the source github page: 

```bash
$ module load gcc/11 cuda/11 cudnn
$ conda create --prefix /projects/tuev/tuev_work/conda_env/LivePortrait python=3.10 pip --y
$ conda activate /projects/tuev/tuev_work/conda_env/LivePortrait
$ pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118
# Note that above torch was installed with cuda 11.8. Follow the guideline from github page to install other cuda version
```

## 2. Clone the github repo to your shared folder and start installing dependencies:

```bash
$ cd /projects/tuev/tuev_work/
$ git clone https://github.com/KwaiVGI/LivePortrait
$ pip install -r requirements.txt
```

## 3. Download pretrained weight:

Here I already install huggingface_hub. If you havent done so, pls do:

```bash
# !pip install -U "huggingface_hub[cli]"
huggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude "*.git*" "README.md" "docs"
```

## 4. Run the model inference via CLI:

- It is very straightforwad to use LivePortrait in CLI. You just need to give the source image and driving video:

```bash
python inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d5.pkl # portrait animation
python inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d5.pkl # portrait video editing
```

- The output can be found from animation folder

## 5. Using GUI via Gradio

### 5.1. Using Gradio Interface via M3:

- If you are using M3, you need to use Virtual Desktop, available from the portal: hpc.m3.smu.edu
- Request Virtual Desktop with gpu_dev partition
- Open Terminal, navigate to the clone github repor in step 2 then run:

```bash
$ python app.py
```

The following screenshot:

![image](https://github.com/user-attachments/assets/6ee7a935-a478-4549-9b2e-55b7a2c911e1)

- Open browser in Virtual Desktop and paste in the link:  http://127.0.0.1:8890
- You will see the GUI opened:

![image](https://github.com/user-attachments/assets/d81dcd4e-3065-4fb5-a604-b8a9682b4b61)

### 5.2. Using Gradio Interface via portforwarding in SPOD

- Follow exactly the same step in Using **SuperPOD** for Grado GUI from terminal
- Basically there are 2 opened CLI:
- CLI1: request GPU node and load cuda/cudnn library, activate conda env then run:

```bash
$ jupyter lab &
$ python app.py
```

- CLI2: from local PC: run the following command:
Note:** 5000** is the port that I setup in Firefox and **8890** is the port from LivePortrait model
```bash
$ ssh -J tuev@superpod.smu.edu tuev@bcm-dgxa100-0003 -L 5000:localhost:5000 -L 8890:localhost:8890
```

- Open Firefox then paste in http://127.0.0.1:8890

### Example output:




https://github.com/user-attachments/assets/304ace57-151b-4e15-9ca8-d5bdae6fedc4


https://github.com/user-attachments/assets/4c3bb80c-e75b-45ee-8491-25b969de77dd


https://github.com/user-attachments/assets/85210143-b232-4ebc-9446-b471220d394b

